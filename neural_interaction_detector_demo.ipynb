{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import bisect\n",
    "import operator\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_main_effect_nets = True # toggle this to use \"main effect\" nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "display_step = 100\n",
    "l1_const = 5e-5\n",
    "num_samples = 30000 #30k datapoints, split 1/3-1/3-1/3\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 140 # 1st layer number of neurons\n",
    "n_hidden_2 = 100 # 2nd layer number of neurons\n",
    "n_hidden_3 = 60 # 3rd \"\n",
    "n_hidden_4 = 20 # 4th \"\n",
    "n_hidden_uni = 10\n",
    "num_input = 10 # simple synthetic example input dimension\n",
    "num_output = 1 # regression or classification output dimension\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_output])\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interaction data generator\n",
    "def synth_func(x):\n",
    "    interaction1 = np.exp(np.fabs(x[:,0]-x[:,1]))                        \n",
    "    interaction2 = np.fabs(x[:,1]*x[:,2])  \n",
    "    interaction3 = -1*np.power(np.power(x[:,2],2),np.fabs(x[:,3])) \n",
    "    interaction4 = np.power(x[:,0]*x[:,3],2)\n",
    "    interaction5 = np.log(np.power(x[:,3],2) + np.power(x[:,4],2) + np.power(x[:,6],2) + np.power(x[:,7],2))\n",
    "    main_effects = x[:,8] + 1/(1+np.power(x[:,9],2))\n",
    "\n",
    "    y =         interaction1 + interaction2 + interaction3 + interaction4 + interaction5 + main_effects\n",
    "    #ground truth:  {1,2}         {2,3}          {3,4}          {1,4}        {4,5,7,8}\n",
    "    return y\n",
    "\n",
    "def gen_synth_data():\n",
    "    X = np.random.uniform(low=-1, high=1, size=(num_samples,10))\n",
    "    Y = np.expand_dims(synth_func(X),axis=1)\n",
    "    \n",
    "    a = num_samples//3\n",
    "    b = 2*num_samples//3\n",
    "    \n",
    "    tr_x, va_x, te_x = X[:a], X[a:b], X[b:]\n",
    "    tr_y, va_y, te_y = Y[:a], Y[a:b], Y[b:]\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_x.fit(tr_x)\n",
    "    scaler_y.fit(tr_y)\n",
    "\n",
    "    tr_x, va_x, te_x = scaler_x.transform(tr_x), scaler_x.transform(va_x), scaler_x.transform(te_x)\n",
    "    tr_y, va_y, te_y = scaler_y.transform(tr_y), scaler_y.transform(va_y), scaler_y.transform(te_y)\n",
    "    return tr_x, va_x, te_x, tr_y, va_y, te_y\n",
    "\n",
    "# Get data\n",
    "tr_x, va_x, te_x, tr_y, va_y, te_y = gen_synth_data()\n",
    "tr_size = tr_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# access weights & biases\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([num_input, n_hidden_1], 0, 0.1)),\n",
    "    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], 0, 0.1)),\n",
    "    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3], 0, 0.1)),\n",
    "    'h4': tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4], 0, 0.1)),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden_4, num_output], 0, 0.1))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.truncated_normal([n_hidden_1], 0, 0.1)),\n",
    "    'b2': tf.Variable(tf.truncated_normal([n_hidden_2], 0, 0.1)),\n",
    "    'b3': tf.Variable(tf.truncated_normal([n_hidden_3], 0, 0.1)),\n",
    "    'b4': tf.Variable(tf.truncated_normal([n_hidden_4], 0, 0.1)),\n",
    "    'out': tf.Variable(tf.truncated_normal([num_output], 0, 0.1))\n",
    "}\n",
    "\n",
    "def get_weights_uninet():\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([1, n_hidden_uni], 0, 0.1)),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_uni, n_hidden_uni], 0, 0.1)),\n",
    "        'h3': tf.Variable(tf.truncated_normal([n_hidden_uni, n_hidden_uni], 0, 0.1)),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_uni, num_output], 0, 0.1))\n",
    "    }\n",
    "    return weights\n",
    "\n",
    "def get_biases_uninet():\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.truncated_normal([n_hidden_uni], 0, 0.1)),\n",
    "        'b2': tf.Variable(tf.truncated_normal([n_hidden_uni], 0, 0.1)),\n",
    "        'b3': tf.Variable(tf.truncated_normal([n_hidden_uni], 0, 0.1))\n",
    "    }\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def normal_neural_net(x, weights, biases):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['h3']), biases['b3']))\n",
    "    layer_4 = tf.nn.relu(tf.add(tf.matmul(layer_3, weights['h4']), biases['b4']))    \n",
    "    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "def main_effect_net(x, weights, biases):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['h3']), biases['b3']))    \n",
    "    out_layer = tf.matmul(layer_3, weights['out'])\n",
    "    return out_layer\n",
    "\n",
    "# L1 regularizer\n",
    "def l1_norm(a): return tf.reduce_sum(tf.abs(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "net = normal_neural_net(X, weights, biases)\n",
    "\n",
    "if use_main_effect_nets:  \n",
    "    me_nets = []\n",
    "    for x_i in range(num_input):\n",
    "        me_net = main_effect_net(tf.expand_dims(X[:,x_i],1), get_weights_uninet(), get_biases_uninet())\n",
    "        me_nets.append(me_net)\n",
    "    net = net + sum(me_nets)\n",
    "\n",
    "# Define optimizer\n",
    "loss_op = tf.losses.mean_squared_error(labels=Y, predictions=net)\n",
    "# loss_op = tf.sigmoid_cross_entropy_with_logits(labels=Y,logits=net) # use this in the case of binary classification\n",
    "sum_l1 = tf.reduce_sum([l1_norm(weights[k]) for k in weights])\n",
    "loss_w_reg_op = loss_op + l1_const*sum_l1 \n",
    "\n",
    "batch = tf.Variable(0)\n",
    "decaying_learning_rate = tf.train.exponential_decay(learning_rate, batch*batch_size, tr_size, 0.95, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=decaying_learning_rate).minimize(loss_w_reg_op, global_step=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Epoch 50\n",
      "\t train rmse 0.03911299422164593 val rmse 0.046781548587818335 test rmse 0.04766639578022324\n",
      "\t learning rate 0.000809947\n",
      "Epoch 100\n",
      "\t train rmse 0.035532514060659194 val rmse 0.043421838204422136 test rmse 0.044302592678017526\n",
      "\t learning rate 6.23213e-05\n",
      "Epoch 150\n",
      "\t train rmse 0.0352709986999008 val rmse 0.043239377950527326 test rmse 0.04399459512626101\n",
      "\t learning rate 4.79531e-06\n",
      "Epoch 200\n",
      "\t train rmse 0.035248727497338636 val rmse 0.04320754406068322 test rmse 0.043997104906904214\n",
      "\t learning rate 3.68974e-07\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_batches = tr_size//batch_size\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.25\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Initialized')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    batch_order = list(range(n_batches))\n",
    "    np.random.shuffle(batch_order)\n",
    "\n",
    "    for i in batch_order:\n",
    "        batch_x = tr_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_y = tr_y[i*batch_size:(i+1)*batch_size]\n",
    "        _, lr = sess.run([optimizer,decaying_learning_rate], feed_dict={X:batch_x, Y:batch_y})\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        tr_mse = sess.run(loss_op, feed_dict={X:tr_x, Y:tr_y})\n",
    "        va_mse = sess.run(loss_op, feed_dict={X:va_x, Y:va_y})\n",
    "        te_mse = sess.run(loss_op, feed_dict={X:te_x, Y:te_y})\n",
    "        print('Epoch', epoch+1)\n",
    "        print('\\t','train rmse', math.sqrt(tr_mse), 'val rmse', math.sqrt(va_mse), 'test rmse', math.sqrt(te_mse))\n",
    "        print('\\t','learning rate', lr)\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_weights(w_dict):\n",
    "    hidden_layers = [int(layer[1:]) for layer in w_dict.keys() if layer.startswith('h')]\n",
    "    output_h = ['h' + str(x) for x in range(max(hidden_layers),1,-1)]\n",
    "    w_agg = np.abs(w_dict['out'])\n",
    "    w_h1 = np.abs(w_dict['h1'])\n",
    "\n",
    "    for h in output_h:\n",
    "        w_agg = np.matmul( np.abs(w_dict[h]), w_agg)\n",
    "\n",
    "    return w_h1, w_agg \n",
    "\n",
    "def get_interaction_ranking(w_dict):\n",
    "    xdim = w_dict['h1'].shape[0]\n",
    "    w_h1, w_agg = preprocess_weights(w_dict)\n",
    "        \n",
    "    # rank interactions\n",
    "    interaction_strengths = dict()\n",
    "\n",
    "    for i in range(len(w_agg)):\n",
    "        sorted_fweights = sorted(enumerate(w_h1[:,i]), key=lambda x:x[1], reverse = True)\n",
    "        interaction_candidate = []\n",
    "        weight_list = []       \n",
    "        for j in range(len(w_h1)):\n",
    "            bisect.insort(interaction_candidate, sorted_fweights[j][0]+1)\n",
    "            weight_list.append(sorted_fweights[j][1])\n",
    "            if len(interaction_candidate) == 1:\n",
    "                continue\n",
    "            interaction_tup = tuple(interaction_candidate)\n",
    "            if interaction_tup not in interaction_strengths:\n",
    "                interaction_strengths[interaction_tup] = 0\n",
    "            inter_agg = min(weight_list)      \n",
    "            interaction_strengths[interaction_tup] += np.abs(inter_agg*np.sum(w_agg[i]))\n",
    "        \n",
    "    interaction_sorted = sorted(interaction_strengths.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    # forward prune the ranking of redundant interactions\n",
    "    interaction_ranking_pruned = []\n",
    "    existing_largest = []\n",
    "    for i, inter in enumerate(interaction_sorted):\n",
    "        if len(interaction_ranking_pruned) > 20000: break\n",
    "        skip = False\n",
    "        indices_to_remove = set()\n",
    "        for inter2_i, inter2 in enumerate(existing_largest):\n",
    "            # if this is not the existing largest\n",
    "            if set(inter[0]) < set(inter2[0]):\n",
    "                skip = True\n",
    "                break\n",
    "            # if this is larger, then need to recall this index later to remove it from existing_largest\n",
    "            if set(inter[0]) > set(inter2[0]):\n",
    "                indices_to_remove.add(inter2_i)\n",
    "        if skip:\n",
    "            assert len(indices_to_remove) == 0\n",
    "            continue\n",
    "        prevlen = len(existing_largest)\n",
    "        existing_largest[:] = [el for el_i, el in enumerate(existing_largest) if el_i not in indices_to_remove]\n",
    "        existing_largest.append(inter)\n",
    "        interaction_ranking_pruned.append((inter[0], inter[1]))\n",
    "\n",
    "        curlen = len(existing_largest)\n",
    "\n",
    "    return interaction_ranking_pruned\n",
    "\n",
    "def get_pairwise_ranking(w_dict):\n",
    "    xdim = w_dict['h1'].shape[0]\n",
    "    w_h1, w_agg = preprocess_weights(w_dict)\n",
    "\n",
    "    input_range = range(1,xdim+1)\n",
    "    pairs = [(xa,yb) for xa in input_range for yb in input_range if xa != yb]\n",
    "    for entry in pairs:\n",
    "        if (entry[1], entry[0]) in pairs:\n",
    "            pairs.remove((entry[1],entry[0]))\n",
    "\n",
    "    pairwise_strengths = []\n",
    "    for pair in pairs:\n",
    "        a = pair[0]\n",
    "        b = pair[1]\n",
    "        wa = w_h1[a-1].reshape(w_h1[a-1].shape[0],1)\n",
    "        wb = w_h1[b-1].reshape(w_h1[b-1].shape[0],1)\n",
    "        wz = np.abs(np.minimum(wa , wb))*w_agg\n",
    "        cab = np.sum(np.abs(wz))\n",
    "        pairwise_strengths.append((pair, cab))\n",
    "#     list(zip(pairs, pairwise_strengths))\n",
    "\n",
    "    pairwise_ranking = sorted(pairwise_strengths,key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    return pairwise_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dict = sess.run(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), 3.9842937443447619),\n",
       " ((1, 4), 1.2878044545665732),\n",
       " ((5, 7), 1.1594737470149994),\n",
       " ((2, 3), 1.1573218256235123),\n",
       " ((7, 8), 1.0395813845098441),\n",
       " ((3, 4), 0.9955278397655154),\n",
       " ((5, 8), 0.74511629942584667),\n",
       " ((5, 7, 8), 0.65411340026184916),\n",
       " ((4, 7), 0.52771079540252686),\n",
       " ((4, 8), 0.51658676937230408),\n",
       " ((4, 5, 8), 0.44517305865883827),\n",
       " ((4, 5, 7, 8), 0.26200527045875788),\n",
       " ((1, 2, 3), 0.059168170135783404),\n",
       " ((1, 2, 4), 0.036433947985880635),\n",
       " ((1, 2, 8), 0.018344591895609306),\n",
       " ((4, 5, 6, 7, 8), 0.018325034528970718),\n",
       " ((3, 4, 8), 0.017842164263129234),\n",
       " ((1, 2, 9), 0.012655940991698011),\n",
       " ((1, 2, 4, 9), 0.012392999654720196),\n",
       " ((1, 4, 5, 7, 8), 0.010827778314705938),\n",
       " ((1, 2, 8, 9), 0.010519576074955239),\n",
       " ((3, 4, 5, 6, 7, 8), 0.0095966905355453491),\n",
       " ((1, 4, 6), 0.0094528580084776905),\n",
       " ((2, 4, 5, 7, 8), 0.0089620146900415421),\n",
       " ((1, 2, 3, 4), 0.0078430417855737296),\n",
       " ((1, 2, 5, 8, 9), 0.0070880441926419735),\n",
       " ((3, 4, 5, 8, 10), 0.0063181919977068901),\n",
       " ((1, 3, 4, 6), 0.0063043271657079458),\n",
       " ((2, 3, 4, 5, 6, 7, 8, 9), 0.0061265081167380074),\n",
       " ((1, 3, 4, 5, 8, 10), 0.0057533769868314266),\n",
       " ((4, 5, 7, 8, 10), 0.0057108770124688476),\n",
       " ((1, 2, 3, 5), 0.0049635478148672973),\n",
       " ((1, 4, 5, 6, 7, 8), 0.0048215012066319552),\n",
       " ((1, 2, 4, 8), 0.0047749599000068178),\n",
       " ((1, 2, 4, 5, 9), 0.004721403097903476),\n",
       " ((1, 4, 5, 6, 7, 8, 9), 0.0045253913849820247),\n",
       " ((1, 2, 3, 8), 0.00376380595844239),\n",
       " ((1, 3, 4, 5, 6), 0.0035509002631023442),\n",
       " ((1, 2, 3, 4, 9), 0.0035398837644606829),\n",
       " ((1, 3, 4, 5, 8, 9, 10), 0.0033599125926002671),\n",
       " ((2, 3, 4, 5, 6, 7, 8, 9, 10), 0.0032059407721369785),\n",
       " ((1, 2, 6), 0.003108019894206393),\n",
       " ((1, 2, 4, 5, 9, 10), 0.0028327601030468941),\n",
       " ((1, 2, 4, 8, 10), 0.0028053943524569708),\n",
       " ((1, 2, 4, 5, 8, 9, 10), 0.0027613772545081955),\n",
       " ((1, 2, 3, 4, 8), 0.0026958026891012871),\n",
       " ((1, 3, 5, 7, 8), 0.002616488141939044),\n",
       " ((1, 2, 3, 4, 5, 8), 0.0025634847116009995),\n",
       " ((1, 2, 4, 5, 6, 7, 8, 9), 0.0025502934587135597),\n",
       " ((1, 2, 3, 4, 5, 8, 9), 0.0022935061715613071),\n",
       " ((1, 3, 4, 5, 7, 8, 10), 0.0022376380511652649),\n",
       " ((1, 2, 3, 4, 9, 10), 0.0022234079442569055),\n",
       " ((1, 3, 4, 5, 6, 7, 8), 0.0021843810100108385),\n",
       " ((1, 2, 3, 4, 5, 6), 0.0017307723414319298),\n",
       " ((1, 2, 3, 4, 5, 6, 8), 0.0014997749822214246),\n",
       " ((1, 2, 3, 4, 5, 7), 0.0014751055277884007),\n",
       " ((1, 2, 3, 4, 5, 6, 8, 10), 0.001312300929652236),\n",
       " ((1, 2, 3, 4, 5, 7, 8, 9), 0.0012155259165509127),\n",
       " ((1, 2, 3, 4, 5, 9, 10), 0.0011754765873797472),\n",
       " ((1, 2, 7, 10), 0.0010162316029891372),\n",
       " ((1, 2, 3, 4, 5, 7, 8, 10), 0.0009266868448539892),\n",
       " ((1, 2, 3, 4, 5, 6, 7, 8, 9), 0.00091907746348100501),\n",
       " ((1, 2, 3, 4, 5, 6, 8, 9, 10), 0.00085513936017886252),\n",
       " ((1, 4, 7, 9, 10), 0.00081861409125849605),\n",
       " ((1, 4, 5, 7, 9, 10), 0.00079995032957521457),\n",
       " ((1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 0.00068608349432713379)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable-Order Interaction Ranking\n",
    "get_interaction_ranking(w_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), 4.0479612),\n",
       " ((5, 8), 1.6183388),\n",
       " ((5, 7), 1.6133682),\n",
       " ((7, 8), 1.5589437),\n",
       " ((1, 4), 1.3692775),\n",
       " ((2, 3), 1.1941909),\n",
       " ((4, 8), 1.0497335),\n",
       " ((3, 4), 1.0476193),\n",
       " ((4, 5), 1.0218934),\n",
       " ((4, 7), 0.83411992),\n",
       " ((1, 3), 0.095073581),\n",
       " ((2, 4), 0.080335304),\n",
       " ((1, 8), 0.070268914),\n",
       " ((3, 5), 0.063574985),\n",
       " ((2, 8), 0.061794207),\n",
       " ((3, 8), 0.059727035),\n",
       " ((1, 5), 0.058769718),\n",
       " ((2, 5), 0.051377397),\n",
       " ((2, 9), 0.049841423),\n",
       " ((4, 6), 0.048084557),\n",
       " ((1, 9), 0.047795787),\n",
       " ((5, 6), 0.047461249),\n",
       " ((6, 8), 0.046787921),\n",
       " ((8, 9), 0.045206197),\n",
       " ((5, 9), 0.044821486),\n",
       " ((4, 9), 0.04005),\n",
       " ((6, 7), 0.039612152),\n",
       " ((5, 10), 0.034415431),\n",
       " ((4, 10), 0.034250751),\n",
       " ((8, 10), 0.033772744),\n",
       " ((2, 7), 0.032941118),\n",
       " ((3, 7), 0.032652456),\n",
       " ((3, 9), 0.028732639),\n",
       " ((1, 7), 0.028270187),\n",
       " ((1, 10), 0.02776891),\n",
       " ((1, 6), 0.027052868),\n",
       " ((3, 6), 0.025879536),\n",
       " ((3, 10), 0.025838438),\n",
       " ((2, 6), 0.022900498),\n",
       " ((7, 9), 0.02238699),\n",
       " ((2, 10), 0.019447444),\n",
       " ((9, 10), 0.016316127),\n",
       " ((6, 9), 0.016039878),\n",
       " ((7, 10), 0.014466666),\n",
       " ((6, 10), 0.0078181922)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pairwise Interaction Ranking\n",
    "get_pairwise_ranking(w_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:workspace]",
   "language": "python",
   "name": "conda-env-workspace-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
